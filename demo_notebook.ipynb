{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd60473",
   "metadata": {},
   "source": [
    "# üéØ Unsupervised Image Descriptors Hackathon\n",
    "## Classical Computer Vision Approaches on STL-10 Dataset\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/unsupervised-descriptors-FlowersCS/blob/main/demo_notebook.ipynb)\n",
    "\n",
    "**Team:** FlowersCS  \n",
    "**Challenge:** Unsupervised image representation learning using classical descriptors  \n",
    "**Dataset:** STL-10 (100k unlabeled + 5k+8k labeled images)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Project Overview\n",
    "\n",
    "This notebook demonstrates a comprehensive evaluation of classical computer vision descriptors for unsupervised image representation learning. We compare:\n",
    "\n",
    "### üåê Global Descriptors\n",
    "- **HOG** (Histogram of Oriented Gradients)\n",
    "- **LBP** (Local Binary Patterns) \n",
    "- **Color Histograms**\n",
    "- **GIST** (Global scene descriptor)\n",
    "\n",
    "### üîç Local Descriptors + Encoding\n",
    "- **SIFT, ORB, BRISK, SURF** + Bag of Words\n",
    "- **SIFT, ORB, BRISK, SURF** + VLAD Encoding\n",
    "- **SIFT, ORB, BRISK, SURF** + Fisher Vectors\n",
    "\n",
    "### üéØ Evaluation Protocol\n",
    "- **Classification performance** (Linear SVM, Random Forest, Logistic Regression)\n",
    "- **Robustness testing** (noise, blur, brightness changes)\n",
    "- **Cross-validation** for reliability\n",
    "- **Efficiency analysis** (speed vs accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b384ef2",
   "metadata": {},
   "source": [
    "## üöÄ Setup and Installation\n",
    "\n",
    "Let's start by setting up the environment and downloading our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install opencv-python scikit-learn scikit-image matplotlib seaborn pandas torchvision pillow\n",
    "\n",
    "# For Google Colab - install additional dependencies\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !apt-get update\n",
    "    !apt-get install -y libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender-dev libgomp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the project repository (replace with your actual repository URL)\n",
    "import os\n",
    "if not os.path.exists('unsupervised-descriptors-FlowersCS'):\n",
    "    !git clone https://github.com/yourusername/unsupervised-descriptors-FlowersCS.git\n",
    "    \n",
    "# Change to project directory\n",
    "%cd unsupervised-descriptors-FlowersCS\n",
    "\n",
    "# Verify project structure\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579da3b1",
   "metadata": {},
   "source": [
    "## üì• Data Download\n",
    "\n",
    "Download and prepare the STL-10 dataset for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572310c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download STL-10 dataset\n",
    "!python scripts/download_data.py\n",
    "\n",
    "# Verify data download\n",
    "!ls -la data/stl10_binary/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69141891",
   "metadata": {},
   "source": [
    "## üé¨ Quick Demo\n",
    "\n",
    "Let's run a quick demonstration with a subset of data to show the complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c8a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quick test with reduced dataset\n",
    "!python run_demo.py --quick-test --global-only\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ QUICK DEMO COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b9b86d",
   "metadata": {},
   "source": [
    "## üìä View Results\n",
    "\n",
    "Let's examine the results from our quick test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77667b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load evaluation results\n",
    "try:\n",
    "    with open('results/evaluation_results.json', 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(\"‚úÖ Results loaded successfully!\")\n",
    "    \n",
    "    # Display configuration\n",
    "    config = results.get('evaluation_config', {})\n",
    "    print(f\"\\nEvaluation Configuration:\")\n",
    "    print(f\"- Descriptors: {config.get('descriptors', 'N/A')}\")\n",
    "    print(f\"- Training samples: {config.get('n_train_samples', 'N/A')}\")\n",
    "    print(f\"- Test samples: {config.get('n_test_samples', 'N/A')}\")\n",
    "    print(f\"- Classes: {config.get('n_classes', 'N/A')}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Results file not found. Please run the demo first.\")\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3796ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance summary table\n",
    "if results:\n",
    "    descriptor_results = results.get('descriptors', {})\n",
    "    \n",
    "    performance_data = []\n",
    "    \n",
    "    for desc_name, desc_result in descriptor_results.items():\n",
    "        if desc_result.get('status') == 'success':\n",
    "            classifiers = desc_result.get('classifiers', {})\n",
    "            feature_info = desc_result.get('feature_info', {})\n",
    "            \n",
    "            for clf_name, clf_result in classifiers.items():\n",
    "                if clf_result.get('status') == 'success':\n",
    "                    metrics = clf_result.get('test_metrics', {})\n",
    "                    performance_data.append({\n",
    "                        'Descriptor': desc_name.replace('_', ' ').title(),\n",
    "                        'Classifier': clf_name.replace('_', ' ').title(),\n",
    "                        'Accuracy': metrics.get('accuracy', 0),\n",
    "                        'F1-Score': metrics.get('macro_f1', 0),\n",
    "                        'Precision': metrics.get('macro_precision', 0),\n",
    "                        'Recall': metrics.get('macro_recall', 0),\n",
    "                        'Dimensions': feature_info.get('dimensions', 0),\n",
    "                        'Time (ms/img)': feature_info.get('avg_time_per_image', 0) * 1000\n",
    "                    })\n",
    "    \n",
    "    if performance_data:\n",
    "        df = pd.DataFrame(performance_data)\n",
    "        \n",
    "        # Sort by accuracy\n",
    "        df = df.sort_values('Accuracy', ascending=False)\n",
    "        \n",
    "        print(\"üèÜ Performance Summary (Top Results):\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Display top 10 results\n",
    "        display_df = df.head(10).round(3)\n",
    "        print(display_df.to_string(index=False))\n",
    "        \n",
    "        # Best performer\n",
    "        best = df.iloc[0]\n",
    "        print(f\"\\nü•á Best Performer: {best['Descriptor']} + {best['Classifier']}\")\n",
    "        print(f\"   Accuracy: {best['Accuracy']:.3f}\")\n",
    "        print(f\"   F1-Score: {best['F1-Score']:.3f}\")\n",
    "        print(f\"   Dimensions: {int(best['Dimensions'])}\")\n",
    "        print(f\"   Speed: {best['Time (ms/img)']:.2f} ms/image\")\n",
    "    else:\n",
    "        print(\"‚ùå No performance data found.\")\n",
    "else:\n",
    "    print(\"‚ùå No results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea183749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance visualization\n",
    "if results and 'performance_data' in locals():\n",
    "    # Performance comparison plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Get best accuracy for each descriptor\n",
    "    best_per_descriptor = df.groupby('Descriptor')['Accuracy'].max().sort_values(ascending=True)\n",
    "    \n",
    "    # Create horizontal bar plot\n",
    "    bars = plt.barh(range(len(best_per_descriptor)), best_per_descriptor.values, \n",
    "                    color=plt.cm.viridis(np.linspace(0, 1, len(best_per_descriptor))))\n",
    "    \n",
    "    plt.yticks(range(len(best_per_descriptor)), best_per_descriptor.index)\n",
    "    plt.xlabel('Accuracy', fontsize=12)\n",
    "    plt.title('Descriptor Performance Comparison\\n(Best Accuracy per Descriptor)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Efficiency vs Accuracy scatter plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Group by descriptor and get best results\n",
    "    best_results = df.loc[df.groupby('Descriptor')['Accuracy'].idxmax()]\n",
    "    \n",
    "    scatter = plt.scatter(best_results['Time (ms/img)'], best_results['Accuracy'], \n",
    "                         s=best_results['Dimensions']/20, alpha=0.7,\n",
    "                         c=range(len(best_results)), cmap='viridis')\n",
    "    \n",
    "    # Add labels\n",
    "    for i, row in best_results.iterrows():\n",
    "        plt.annotate(row['Descriptor'], \n",
    "                    (row['Time (ms/img)'], row['Accuracy']),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=10, alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Time per Image (ms)', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.title('Efficiency Analysis: Accuracy vs Speed\\n(Bubble size = Feature Dimensions)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7626046e",
   "metadata": {},
   "source": [
    "## üéØ Full Evaluation (Optional)\n",
    "\n",
    "Run the complete evaluation with all descriptors (this will take longer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run for full evaluation\n",
    "# Warning: This will take 15-30 minutes depending on hardware\n",
    "\n",
    "# !python run_demo.py --max-samples 5000\n",
    "\n",
    "print(\"To run full evaluation, uncomment the line above.\")\n",
    "print(\"This will train and evaluate all descriptors with 5000 samples.\")\n",
    "print(\"Expected runtime: 15-30 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4e83f",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Sample Images and Features\n",
    "\n",
    "Let's visualize some sample images from the STL-10 dataset and their extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9982274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('src')\n",
    "\n",
    "try:\n",
    "    from utils.data_loader import STL10Dataset\n",
    "    from utils.preprocessing import ImagePreprocessor\n",
    "    \n",
    "    # Load sample data\n",
    "    dataset = STL10Dataset('data')\n",
    "    X_test, y_test = dataset.load_test_data()\n",
    "    \n",
    "    # Class names\n",
    "    class_names = ['airplane', 'bird', 'car', 'cat', 'deer', \n",
    "                   'dog', 'horse', 'monkey', 'ship', 'truck']\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(X_test)} test images\")\n",
    "    print(f\"Image shape: {X_test.shape[1:]}\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    X_test, y_test = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc19faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "if X_test is not None:\n",
    "    # Select one image per class\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, class_idx in enumerate(range(10)):\n",
    "        # Find first image of this class\n",
    "        class_mask = y_test == class_idx\n",
    "        if np.any(class_mask):\n",
    "            img_idx = np.where(class_mask)[0][0]\n",
    "            img = X_test[img_idx]\n",
    "            \n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'{class_names[class_idx]}\\n(Class {class_idx})', fontsize=10)\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, 'No samples', ha='center', va='center', \n",
    "                        transform=axes[i].transAxes)\n",
    "            axes[i].set_title(f'{class_names[class_idx]}\\n(No samples)', fontsize=10)\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('STL-10 Dataset Sample Images (One per Class)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show class distribution\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar([class_names[i] for i in unique], counts, \n",
    "                   color=plt.cm.tab10(np.linspace(0, 1, len(unique))))\n",
    "    \n",
    "    plt.title('Class Distribution in Test Set', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Classes', fontsize=12)\n",
    "    plt.ylabel('Number of Images', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b0c0b",
   "metadata": {},
   "source": [
    "## üîç Feature Extraction Demo\n",
    "\n",
    "Let's demonstrate feature extraction with one of our descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d460eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate feature extraction\n",
    "if X_test is not None:\n",
    "    try:\n",
    "        from descriptors.global_descriptors import HOGDescriptor\n",
    "        \n",
    "        # Initialize HOG descriptor\n",
    "        hog = HOGDescriptor()\n",
    "        \n",
    "        # Take a small sample\n",
    "        sample_images = X_test[:5]\n",
    "        sample_labels = y_test[:5]\n",
    "        \n",
    "        # Fit and extract features\n",
    "        print(\"Extracting HOG features...\")\n",
    "        hog.fit(sample_images)\n",
    "        features = hog.extract(sample_images)\n",
    "        \n",
    "        print(f\"‚úÖ Features extracted successfully!\")\n",
    "        print(f\"Feature matrix shape: {features.shape}\")\n",
    "        print(f\"Feature vector length: {features.shape[1]}\")\n",
    "        print(f\"Feature range: [{features.min():.3f}, {features.max():.3f}]\")\n",
    "        \n",
    "        # Visualize feature distributions\n",
    "        plt.figure(figsize=(15, 4))\n",
    "        \n",
    "        # Feature histogram\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.hist(features.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.title('HOG Feature Value Distribution')\n",
    "        plt.xlabel('Feature Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        # Feature vector for first image\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(features[0], alpha=0.7)\n",
    "        plt.title(f'HOG Features for First Image\\n(Class: {class_names[sample_labels[0]]})')\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Feature Value')\n",
    "        \n",
    "        # Feature similarity matrix\n",
    "        plt.subplot(1, 3, 3)\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        similarity = cosine_similarity(features)\n",
    "        \n",
    "        im = plt.imshow(similarity, cmap='viridis')\n",
    "        plt.title('Feature Similarity Matrix\\n(Cosine Similarity)')\n",
    "        plt.xlabel('Image Index')\n",
    "        plt.ylabel('Image Index')\n",
    "        plt.colorbar(im)\n",
    "        \n",
    "        # Add labels\n",
    "        labels = [class_names[label] for label in sample_labels]\n",
    "        plt.xticks(range(len(labels)), labels, rotation=45)\n",
    "        plt.yticks(range(len(labels)), labels)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Feature extraction demo failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No data available for feature extraction demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51d499",
   "metadata": {},
   "source": [
    "## üìù Summary and Conclusions\n",
    "\n",
    "Based on our experiments with classical computer vision descriptors on the STL-10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c25e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ HACKATHON RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìä PROJECT HIGHLIGHTS:\")\n",
    "print(\"  ‚Ä¢ Implemented 8 classical descriptors (4 global + 4 local)\")\n",
    "print(\"  ‚Ä¢ Tested 3 encoding methods for local descriptors\")\n",
    "print(\"  ‚Ä¢ Evaluated with 3 different classifiers\")\n",
    "print(\"  ‚Ä¢ Comprehensive robustness testing\")\n",
    "print(\"  ‚Ä¢ Complete automation and reproducibility\")\n",
    "\n",
    "print(\"\\nüèÜ KEY FINDINGS:\")\n",
    "if 'df' in locals() and len(df) > 0:\n",
    "    best = df.iloc[0]\n",
    "    print(f\"  ‚Ä¢ Best performer: {best['Descriptor']} ({best['Accuracy']:.1%} accuracy)\")\n",
    "    print(f\"  ‚Ä¢ Most efficient: Feature dimensions from {df['Dimensions'].min():.0f} to {df['Dimensions'].max():.0f}\")\n",
    "    print(f\"  ‚Ä¢ Speed range: {df['Time (ms/img)'].min():.1f} - {df['Time (ms/img)'].max():.1f} ms per image\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ Run full evaluation to see detailed results\")\n",
    "\n",
    "print(\"\\nüí° TECHNICAL INSIGHTS:\")\n",
    "print(\"  ‚Ä¢ Global descriptors provide good baseline performance\")\n",
    "print(\"  ‚Ä¢ Local descriptors with encoding can achieve higher accuracy\")\n",
    "print(\"  ‚Ä¢ Trade-offs exist between accuracy, speed, and feature dimensions\")\n",
    "print(\"  ‚Ä¢ Classical methods still competitive for many vision tasks\")\n",
    "\n",
    "print(\"\\nüöÄ REPRODUCIBILITY:\")\n",
    "print(\"  ‚Ä¢ Complete code available on GitHub\")\n",
    "print(\"  ‚Ä¢ One-command execution: python run_demo.py\")\n",
    "print(\"  ‚Ä¢ Comprehensive test suite included\")\n",
    "print(\"  ‚Ä¢ Google Colab ready for easy testing\")\n",
    "\n",
    "print(\"\\nüìÅ OUTPUT FILES:\")\n",
    "print(\"  ‚Ä¢ results/evaluation_results.json - Detailed metrics\")\n",
    "print(\"  ‚Ä¢ results/evaluation_report.txt - Summary report\")\n",
    "print(\"  ‚Ä¢ results/visualizations/ - Performance plots\")\n",
    "print(\"  ‚Ä¢ cache/ - Trained models for reuse\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚ú® Thank you for exploring our hackathon project! ‚ú®\")\n",
    "print(\"üîó Full code: https://github.com/yourusername/unsupervised-descriptors-FlowersCS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec465e1",
   "metadata": {},
   "source": [
    "## üéØ Next Steps and Extensions\n",
    "\n",
    "To extend this work, consider:\n",
    "\n",
    "### üî¨ Advanced Techniques\n",
    "- **Deep features**: Pre-trained CNN features (ResNet, VGG)\n",
    "- **Hybrid approaches**: Combining classical and deep features\n",
    "- **Advanced encodings**: NetVLAD, FisherNet\n",
    "\n",
    "### üìà Evaluation Extensions\n",
    "- **Retrieval tasks**: Image similarity and search\n",
    "- **Few-shot learning**: Performance with limited labels\n",
    "- **Domain transfer**: Cross-dataset evaluation\n",
    "\n",
    "### ‚ö° Optimization\n",
    "- **GPU acceleration**: CUDA-optimized feature extraction\n",
    "- **Quantization**: Reduced precision for deployment\n",
    "- **Ensemble methods**: Combining multiple descriptors\n",
    "\n",
    "---\n",
    "\n",
    "**Team FlowersCS** - Demonstrating that classical computer vision methods remain valuable tools in the modern ML toolkit! üå∏"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
